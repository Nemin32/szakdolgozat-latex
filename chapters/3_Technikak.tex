A tervezett értelmezőnek képesnek kell lennie a felhasználó által szolgáltatott nyers szövegen olyan átalakításokat végeznie, melynek végeredménye egy megállítható és léptethető programkód.

Ehhez a következő lépéseket kell elvégeznünk

\subsection{Formázás}
\label{sec:formatting}

Bár a C-szerű nyelvekben, így a Pszeudokódban is, az üres helyek és sortörések csupán a különféle nyelvi elemek elhatárolására szolgálnak, mégis a kód könnyű átláthatósága érdekében definiálni szoktunk egy szabályrendszert, amely alapján a kód különböző elemeit formázzuk.

Rengetegféle variáció létezik az ilyen szabályrendszerekre, beleértve, hogy hová helyezzük a kapcsos-zárójeleket vagy hány üres helyet (vagy esetleg tabulátor karaktert) helyezzünk az egyes sorok elé. A C-nyelv alkotói például így vélekednek a könyvükben:

\say{A kapcsos-zárójelek elhelyezése kevésbé fontos, ám az embereknek szenvedélyes meggyőződéseik vannak velük kapcsolatban. Mi az egyik közkedvelt stílust választottuk. Válassz egy olyat, ami számodra megfelelő, aztán tartsd hozzá magad.}\cite{kr}

Épp emiatt átfogó algoritmust, technikát, de akárcsak szabályrendszert is megnevezni ehhez a lépéshez lehetetlen volna. Ehelyett én csupán azt definiálnám itt, hogy az általam kialakított stílusra a következő szabályok vonatkoznak:

\begin{itemize}
    \item Az egyes behúzások két üres helynek felelnek meg,
    \item Az egyes egymástól elhatárolt blokkok közé (függvények, elágazások, ciklusok) sortörést helyezünk,
    \item Az egymást követő utasítások közé nem helyezünk sortörést, kivéve ha az előző szabály szerint tennénk,
    \item A kód se az elején, se a végén nem tartalmaz fölösleges üres helyeket vagy sortöréseket,
    \item Az egyes sorok nem tartalmaznak se az elejükön, se a végükön fölösleges üres helyeket,
    \item Ha új blokkot kezdeményező utasítással kezdődik a sor (például "ciklus", "függvény", "ha", stb.), akkor a jelenlegi sort követő kód behúzásra kerül,
    \item Ha blokkot záró utasítással végződik egy sor ("[utasítás] vége"), akkor már a jelenlegi sort és az azokat követőket egy szinttel kevésbé húzzuk be.
\end{itemize}

Ezeket a szabályokat alkalmazva a felhasználói bemenettől függetlenül konzisztens és jól tördelt kódot kapunk, melyet továbbadunk a következő lépésnek.

\subsection{Tokenizálás}
\label{sec:tokenizer_tech}

Robert Nystrom a \textit{Crafting Interpreters}\cite{crafting} című könyvének tokenizálásról szóló fejezetét a következővel nyitja:

\say{Az első lépés bármely értelmező vagy fordító létrehozásában a szkennelés. A szkenner a nyers bemeneti forráskódot karakterek sorozataként fogadja, majd ezeket tokeneknek nevezett csoportokba gyűjti. Ezek a nyelv értelmes \say{szavai} és \say{írásjelei} melyek kialakítják annak nyelvtanát.}

Ahogy az fentebb is olvasható, a nyers szöveg hiába van jól formázva, önmagában a számítógép számára továbbra is egy egybefüggő, értelmezhetetlen adat. Ezt orvosolni két lehetőségünk van:

Vagy feldolgozhatjuk a szöveget úgy ahogy van. Erre főleg egyszerűbb nyelveknél van lehetőségünk, ahol nagyon kevés és egyszerű utasításokkal dolgozunk, vagy esetleg olyan környezetben, ahol a további átalakítások memóriaigénye nem kielégíthető.

Jó példa erre a FORTH, mely egy minimalista, verem-alapú programozási nyelv, aminek értelmezéséhez nincs szükség a bemenet előzetes feldolgozásra, csupán egy függvényre, mely a szöveg következő szavát (vagyis azt a karakterláncot, ami a következő üres helyig tart) képes beolvasni egy pufferbe. Természetesen ez a módszer bár igen egyszerű, mellé erősen limitált és céljaink számára teljesen alkalmatlan, így többet nem is foglalkozunk vele.

Helyette a nyers szöveget a fordítóprogram által is értelmezni képes legkisebb egységekre (úgynevezett "tokenekbe") bontjuk. Ez a folyamat a tokenizálás.\footnote{A szakirodalmat böngészve több különböző kifejezéssel is találkozhatunk erre a folyamatra. A fentebb idézett szövegben az író a "szkennelés" szót alkalmazta, míg én a "tokenizálást". Megint más szövegek pedig a "lexer" vagy "lexing" kifejezéssel hivatkoznak a forrásszövegek egységekre való bontására. Szigorúan véve van különbség a fogalmak között (a tokenizáló csupán elemekre bontja a szöveget, míg egy lexer egyéb információkat is fűz az elemekhez), ám általánosságban szinonímaként használjuk őket.}

A tokenek közé tartoznak például az operátorok, a változónevek, a különböző utasítások és kifejezések nevei, a különféle zárójelek, és a már tovább nem bontható, önmagukban is értelmes értékek, az úgynevezett "atomok". A Pszeudokód nyelvben a számokat, szövegeket, és logikai értékeket értjük atomok alatt.

Ugyanígy ebben a lépésben szűrjük ki a jelentéssel nem rendelkező elemeket (úgynevezett \textit{triviákat}). Ezek közé tartoznak a kommentek (hisz ezeket az értelmező figyelmen kívül hagyja) és az jelentést nem módosító üres helyek (mivel a program szempontjából nem okoz semmiféle különbséget, hogy valaki \verb|5 + 3|-t ír vagy \verb|5     +3|-t vagy akár mindhárom karaktert különböző sorba írja).

Kód szempontjából a tokenek olyan adatszerkezetek, melyek ezeket a legkisebb értelmes egységeket tartalmazzák, valamiféle extra információval. Bár minden nyelvben kicsit más az eltárolt információ, van pár gyakori adattag, mely majdnem mindenhol megjelenik. Ezek a következők:

\begin{itemize}
    \item Saját típusuk, mely megmondja a token milyen értéket is tartalmaz (például atomot vagy kulcsszót),
    \item A bemeneti szöveg releváns részletét (például "elágazás"), melyet a következő feldolgozási lépésben fogunk felhasználni komplexebb adatszerkezetek felépítésére,
    \item A token kezdetének sor-oszlop párosát, melyet hibák esetén tudunk a felhasználó számára megjeleníteni, így segítvén a hibakeresést.
\end{itemize}

A tokenizálás megvalósítása általában kétféleképp történik. Ha a nyelv, melyben az értelmezőnket írjuk, támogatja a reguláris kifejezéseket, akkor elegendő csupán minden tokentípusra egy ilyen kifejezést írnunk, majd a mintákat egyesével végigpróbálgatnunk. Ellenkező esetben nagyon hasonló az eljárás, ám itt manuálisan a bemenet karaktereinek egyesével való feldolgozásával tudjuk eldönteni, hogy milyen típusú tokent olvasunk éppen.

A saját programomban ezt az utóbbi módszert választottam, részletes leírását a \ref{sec:tokenizer} alfejezetben tárgyalom.

\subsection{Értelmezés}

Sikeres tokenizálás esetén az így kapott listából egy "absztrakt szintaxisfának" (angolul \textit{"abstract syntax tree"}, melyet gyakran 'AST'-nek rövidítünk) nevezett adatszerkezetet kell építenünk. Ez egy olyan rekurzív fa adatszerkezet, aminek a levelei atomi értékek, az ágai pedig olyan műveletek és utasítások, melyek egy vagy több saját ággal vagy levéllel rendelkeznek.

Az szintaxisfa mellőz minden olyan információt, ami a programozó számára átláthatóbbá teszi a kódot, de a fordító számára nem szükséges a kód értelmezéséhez. Cserébe viszont hierarchikusan rendezi a kód egyes utasításait, megőrizve azok sorrendjét.

Ezen átalakítás elvégzéséhez használjuk az úgynevezett parser\footnote{A "parser" szót a "nyelvi értelmező" kifejezéssel lehetne a legpontosabban magyarra fordítani. Ám, mivel a külföldi szakirodalomban is mindig a "parsing" / "parser" kifejezésekkel hivatkoznak a folyamatra és algoritmus-családra és mivel a magyar név kissé körülményes, ebben a szövegben én is az angol szóval fogom ezeket a fogalmakat illetni.} algoritmusokat. Ahogy neve is sugallja, az algoritmus értelemzi az egymást követő tokenekben rejlő mintákat és ezek alapján komplexebb adatszerkezeteket alkot.

Két fő ágazatot különböztetünk meg, ezek a fentről-lefelé és a lentről-fölfelé parserek.

Az előbbi balról-jobbra dolgozza magát végig a tokenek listáján és egy bizonyos kezdő-minta alapján próbálja feldolgozni az összes tokent, amíg egy érvényes szintaxisfát nem kapunk. Például, ha a parser egy függvény nevét találja, utána megpróbál egy zárójelek közé zárt listát is keresni, melyek a függvény paramétereit tartalmazzák. Ezeket a paramétereket egyesével ismét más minták alapján értelmezi és így tovább, míg a parser előbb vagy utóbb eljut egy atomi elemhez, mely önmagában is értelmes. Ekkor, ha a bemenet nem tartalmazott hibát, egy érvényes szintaxisfa-águnk van, mely tartalmazza a függvényhívást és annak paramétereit gyermek elemekként, melyek esetleg szintén saját gyermek elemeket tartalmaznak. 

Az ilyen parsereket legtöbbször egymást-hívó rekurzív függvényekkel valósítunk meg, ennek neve rekurzív alászálló parser. Alternatív megközelítés még az úgynevezett PEG parser is, mely a reguláris kifejezések kiterjesztése parserekre. Tokenek helyett szövegeket dolgoz fel. Előnye az egyszerűbb rekurzív parserhez képest, mely előre megszabott (általában egy) tokent vesz figyelembe, hogy bárhány karaktert képes egyszerre megvizsgálni, így tetszőleges komplexitású ágakat képes építeni egy lépésben.

A lentről-fölfelé parser esetében pedig a folyamat pont fordítva történik. A parser először az atomi értékeket keresi meg, majd ezekből épít egyre komplexebb rész-fákat, míg végül az egész kód feldolgozásra nem került.

A saját implementációmban egy fentről-lefelé parsert valósítok meg. Ez a \ref{sec:parser} alfejezetben kerül kifejétsre.

\subsection{Típusellenőrzés}
\label{sec:typecheck_tech}

Az elkészült szintaxisfánk garantáltan szintaktikailag helyes kódot tartalmaz, ám arról, hogy szemantikailag is helyes, tehát van értelme annak, amit a kód tenni próbál, szintén meg kell győződnünk.

Formálisabban definiálva, "értelmes program" alatt olyan programot értünk, mely nem végez olyan műveletet, ami áthágja a nyelv utasításainak típusmegkötéseit. Például egy számot nem adhatunk egy szöveghez. Egy elágazás predikátuma mindig logikai érték. Ha egy függvény szám típusú értéket vár, ne adhassunk neki egy tömböt.

Ennek folyamata a típusellenőrzés. Ezt legtöbbször egy rekurzív függvénnyel valósítjuk meg, mely a szintaxisfa minden elemére, legyen az ág vagy levél, képes kiszámolni annak típusát. Mivel a levélelemek atomi értékek és az atomi értékek típusa ismert (például egy szám csak szám típusú lehet), így ezek szolgálnak a rekurzív folyamat alapeseteként és ezekből tudjuk felépíteni az egymásra épülő egyre nagyobb rész-fák típusait is, míg végül elérünk a gyökérelemhez.

Ez a lépés nem végez átalakítást a szintaxisfán, csupán amiatt érdekes számunkra, hogy sikeres végigfutás esetén biztosak lehetünk benne, hogy a kódunk szemantikailag helyes. Hiba esetén pedig információt nyújt arról a kód melyik részletében keressük a rossz utasítást.

Például, ha az összeadás műveletét ellenőrizzük, akkor meg kell néznünk, hogy a bal és jobb oldali elem szám típusú értékek-e (melyeket a függvény rekurzív meghívásával érünk el) és, ha azok, akkor a művelet eredménye is szám lesz (mely tényt esetleg egy másik típusellenőrzésnél is felhasználhatunk).

Általában az így kapott típusbírálatokat rögtön fel is tudjuk használni (például a fent említett összeadás bal és jobb oldali elemének típusellenőrzése nem befolyásolja a folyamat további mozzanatait), ám bizonyos esetekben el kell tárolnunk ezeket. Ilyenek a változók és a függvények ellenőrzéséből kapott típusok, hisz mindkettő példányai többször megjelenhetnek az ellenőrzött kódunkban és meg kell bizonyosodnunk afelől, hogy egy korábban bizonyos típusra beállított változó vagy függvény meghívása nem szegi meg később a nyelv típusmegkötéseit.

A típusellenőrzés technikai részleteiről a \ref{sec:typecheck} alfejezetben beszélek.

\subsection{Szintaxisfa lineáris kódba bontása}

Ha a folyamat idáig jut, akkor biztosak lehetünk benne, hogy egy mind szintaktikailag, mind szemantikailag érvényes és értelmes szintaxisfával rendelkezünk.

Ha a feladatunk csupán ennek lefuttatása volna, akkor a szintaxisfánk futtatásra kész. Hisz a szintaxisfánk egy fa, melynek minden ága is egy értelmes szintaxisfának felel meg, így alkalmas stratégia, hogy egy rekurzív, a fát mélységi-bejárással feldolgozó interpreter segítségével egészen addig osztogatjuk fel részproblémákra a futtatást, míg a levelekig nem érünk, melyek nem igényelnek semmiféle feldolgozást, tehát a rekurzió alapesetének felelnek meg. Innentől pedig a folyamat elkezd visszafejleni és a levelek fölötti ágak eredményei válnak az új levelekké.

Erre a \ref{fig:aritmast} ábra mutat be egy egyszerű példát, mely a 10 / 2 * 15 + 5 egyenletet számolja ki a szintaxisfa közvetlen feldolgozásával.

\addimage{ast.png}{Példa egy absztrakt szintaxisfára.}{aritmast}

A megvalósításom első verziójában én is ezt a verziót választottam és hamar el is érkeztem egy olyan állapothoz, melyben a kód (típusellenőrzés nélkül) értelmezésre került, majd a program egy interpreter segítségével le is futott. Ekkor azonban beláttam a módszer hátrányát, ami pedig az, hogy mivel ez egy rekurzív, fa-alapú bejárás, így nincs módunk arra, hogy egy szálon futó program esetén a folyamatot tetszőleges ponton megállíthassuk. Ez viszont ütközik a projekt céljával, hisz olyan értelmezőt szeretnék írni, mely belső állapotának változását a felhaználó akár minden utasítás lefutása után megvizsgálhatja.

Ehelyett a jelenlegi megvalósításom egy úgynevezett bájtkód interpreter. Ennek lényege, hogy a szintaxisfát a fentebb említett mélységi-bejárás módszerével feldolgozzuk, ám lefuttatás helyett egy újabb kódot generáltatunk belőle. Ezen kód olyan utasításokból áll, melyek nem függenek egymástól és egy tömb adatszerkezetben kerülnek eltárolásra, így az index elmentésével triviális megjegyeznünk meddig is futott már le a kód.

A nem tanítási jelleggel készült nyelvekben a bájtkód, ahogy azt a neve is sugallja, általában egy bájtokból álló utasításokat tartalmazó kód, melyet az értelmező sokkal gyorsabban képes beolvasni és feldolgozni mint egy szöveges fájlt. Azonban mivel az én célom az oktatás elősegítése és az értelmező sebessége másodlagos cél, így az általam generált bájtkód sokkal inkább hasonlít egy Assembly-nyelven írt programhoz, hisz az egyes utasítások olvasható nevekből és atomi értékekből álló paraméterekből épülnek fel. 

Harmadik lehetőség lenne még, hogy bájtkód helyett gépi kódot generáltatok, melyet a számítógép egyéb futtatási környezet nélkül képes értelmezni, majd ezt valamely hibakereső (például GDB vagy LLDB) segítségével teszem megvizsgálhatóvá a felhasználó számára. A leggyakrabban használt, sebességkritikus programozási nyelvek, mint például a C, a C++, a Rust mind ezt a módszert használják. Ez a módszer viszont egyrészt nagyban megbonyolítaná a projektet (és az internetes elérést is sokkal komplexebbé tenné), másrészt a megértésben se segítene, hisz a kezdő programozók nem arról tanulnak, hogyan is működik a számítógép a legalacsonyabb szintjén, hanem, hogy egyáltalán az algoritmusok maguk hogy működnek. Így ezt a módszert elvetettem.

Az bájtkód generálás részletes bemutatása a \ref{sec:compiler} alfejezetben történik.

\subsection{Futtatás}

Az így kapott bájtkód végre futtatható állapotban van, ám, ahogy azt az előző szekcióban is tárgyaltuk, valós gépi kód helyett egy képzeletbeli számítógép architektúrájára fordítottuk a szintaxisfánk.

Ezen képzeletbeli gép szimulátora az úgynezevett "virtuális gép". Ennek feladata, hogy olyan futtatási környezetet biztosítson, mely képes a bemeneti kódot egy specifikált szabályrendszer szerint lefuttatni.

Egy átlagos virtuális gép a következő elemekből áll össze:

\begin{itemize}
    \item Egy \texttt{switch} elágazás, mely a bájtkód összes lehetséges fajtájára rendelkezik egy elágazással,
    \item Egy tetszőleges adatszerkezet, mely emulálja a program által használt szabadon írható és olvasható memóriát,
    \item És egy mechanizmus, mely lehetővé teszi a virtuális gép számára, hogy az őt futtató környezettel kommunikáljon (legyen ez a felhasználó bemenetének továbbítása vagy a kimenet továbbítása a felhasználó felé).
\end{itemize}

A virtuális gépes megvalósítás előnye, hogy teljes mértékben mi szabjuk meg annak belső működését, így például egyszerűen a felhasználó elé tudjuk tárni a futtatási környezet állapotát (milyen változók vannak, mi a memória tartalma, hányadik sor kódnál járunk), mely a projekt szempontjából kifejezetten kívánatos tulajdonság.

Mivel a projektspecifikáció része, hogy a kód egyes utasításai között léptetni is lehessen előre- és visszafelé is, emiatt a gép állapotát úgynevezett megváltoztathatatlan („immutable”) adatszerkezetekben eltárolni, melyek lényege, hogy adattagjai az adatszerkezet létrehozása után nem módosíthatóak. 

Ha ezeket egy listában tároljuk el, a lista feje mindig az éppen aktív állapota a gépünknek. Előreléptetéskor egy új elemet számolunk ki, mely ezáltal a lista új fejévé válik, visszaléptetéskor pedig a tömb egy korábbi elemére lépnünk.

\subsection{Felhasználói felület}

A felhasználói felület feladata, hogy interfészként szolgáljon az előző alfejezetekben vázolt folyamatokhoz, azokat meghjvja, és eredményeiket a felhasználó számára láthatóvá tegye.

Ez célunktól függően lehet automatikus (mely esetén a felhasználó csupán elindítja a folyamatot, esetleg úgy, hogy a program indulásakor bizonyos paramétereket beállít) vagy pedig interaktív (mely esetén a program bizonyos utasítások esetén megáll és a felhasználótól kapott bemenet alapján folytatja működését). Megjelenítés szempontjából készíthetünk parancssori programot, grafikus interfészt, vagy interneten át szolgált webes applikációt.

Én ezutóbbit választottam, mivel a böngészőkben futó programok egységes környezetet nyújt operációs rendszertől és architektúrától függetlenül, így elég csupán egy változatban lefejleszteni a programom, mely így mindenhol működik.

Emelett a végfelhasználó számára is sokkal kényelmesebb, ha csupán el kell navigálnia egy weboldalra, mely egyéb előzetes lépés nélkül rögtön képes értelmezni és végrehajtani az általa begépelt Pszeudokódot.

Végül a böngészőkben futó kód garantáltan úgynevezett \textit{homokozóban} (sandbox) kerül végrehajtásra, mely annyit jelent, hogy a kód és az operációs rendszer között erős válaszfal van, mely megakadályozza, hogy egy rosszindulatú kód bármiféle kárt okozhasson a felhasználó gépében.

Bár az én fordítóprogramom értelmszerűen nem tartalmaz magában kártékony kódot, a böngésző homokozójának felhasználásával biztosra megyek, hogy a legrosszabb, ami történhet egy hibás vagy kártékony bemenet esetén az a böngésző ablakának lefagyasztása.